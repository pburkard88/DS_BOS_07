{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Intro to Classification with KNN\n",
    "Of the several different major classes of machine learning problems, the ones we'll investigate most extensively in this course are [Classification Problems](https://en.wikipedia.org/wiki/Statistical_classification).  In a **classification problem** the **target variable** is **categorical** and known (**supervised**), and thus the task amounts to **classifying** an observation (set of features) into the proper **category** or **class**.  Hence the learning objective is to determine a function that performs this task (mapping feature vectors to categories) with suitable accuracy.\n",
    "\n",
    "In these exercises we'll go through 2 different datasets and perform a specific type of classification algorithm, **KNN (K-Nearest Neighbors)**, to build classification models both manually and using built-in classifiers from `sklearn`.\n",
    "\n",
    "##Learning Goals\n",
    "The packages and concepts we'll hope to get comfortable with in this activity are the following:\n",
    "- [***Classification Problems***](https://en.wikipedia.org/wiki/Statistical_classification): you should understand what the classification task is and how to carry out a classification pipeline start to finish\n",
    "- [***KNN with sklearn***](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html): built-in K-Nearest Neighbors Classifier class in sklearn\n",
    "- [***Feature Standardization***](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html): sklearn and pandas functionality for scaling/standardizing/normalizing numeric feature data\n",
    "- ***Image Features***: So far we've worked with rows of data tables, now we'll just be given images as input and learn some ways to extract structured feature data from these images\n",
    "- [***KNN - How it works***](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm): You should be able to manually carry out a KNN classifier and be familiar with concepts such as different distance metrics and weighting functions\n",
    "- [***Curse of Dimensionality***](https://en.wikipedia.org/wiki/Curse_of_dimensionality): issues that can arise with high-dimensional feature spaces (many features)\n",
    "\n",
    "##Datasets\n",
    "We'll take a look at 2 different datasets:\n",
    "1. [Fisher Iris Dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)  \n",
    "2. [MNIST Database of Handwritten Digits](http://yann.lecun.com/exdb/mnist/)  \n",
    "\n",
    "##Setup\n",
    "Try running the following imports and check that they all import successfully.  If they do not, the commands for the installs are given below.  If necessary, at a command line window run the ones that are failing for you and then retry the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from seaborn import plt\n",
    "from sklearn.datasets import load_iris\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Installations (if necessary)\n",
    "<pre><code>conda install pandas</code></pre>\n",
    "<pre><code>conda install numpy</code></pre>\n",
    "<pre><code>conda install seaborn</code></pre>\n",
    "<pre><code>conda install scikit-learn</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Fisher Iris Dataset\n",
    "For our first few exercises we'll work with the [Fisher Iris Dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) to demonstrate how **KNN Classification** might be done in `sklearn`.  The Iris Dataset contains 4 different numerical features (petal and sepal length and width (cm)) for 3 different species of flowers and the goal is to classify each observation into the proper of the 3 possible species. \n",
    "\n",
    "The Iris Dataset is an old and extremely common dataset in demonstrating machine learning, so much so that it's actually included in `sklearn`.  Let's load it in and explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the Iris Dataset\n",
    "iris = load_iris()\n",
    "# What is the type of this object?\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out some of the properties in a `sklearn.datasets.base.Bunch` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the names of the features\n",
    "feature_names = iris.feature_names\n",
    "# Print the names of the features\n",
    "print 'Feature Names: ' + str(feature_names)\n",
    "# Retrieve the feature data\n",
    "features = iris.data\n",
    "# Print the type of this object\n",
    "print \"Type of 'features': \" + str(type(features))\n",
    "# Print the feature data (a numpy array)\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the target variable data\n",
    "target = iris.target\n",
    "# Print the type of this object\n",
    "print \"Type of 'target': \" + str(type(target))\n",
    "# Print the target data (a numpy array)\n",
    "print target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the `iris` object contains both the feature data and target data stored as `numpy` arrays, and the feature names stored as a `List` of strings.  Let's combine these items to create a `pandas` Dataframe for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a pandas dataframe from the feature data and feature names\n",
    "df = pd.DataFrame(features, columns=feature_names)\n",
    "# Add the target variable to the dataframe\n",
    "df['Target'] = iris.target\n",
    "# Use head to take a look\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Exploring the Iris Data\n",
    "If everything looks good in your dataframe, let's start exploring the data.\n",
    "\n",
    "First let's take a look at the datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check datatypes with info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense, the 4 measurements are floats and the class/target variable is an indicator int representing the flower class.  Let's check out the unique target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the unique values of 'Target'\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, we have 3 unique values of the 'Target' variable, one representing each possible class (species of flower).\n",
    "\n",
    "Now let's use `seaborn` to generate some plots to explore the data further.  For each possible pair of attributes we want to generate a color-coded scatterplot of the observations with different colors for each class.\n",
    "\n",
    "**Q: How many plots does this mean we should have and why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the figure and axes\n",
    "#we'll plot 2x3 figures (why?)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "# Define the different colors to use for the 3 classes\n",
    "colors = ['r','g','b']\n",
    "\n",
    "# For each desired plot, create a scatterplot with the appropriate pair of attributes and the proper axis\n",
    "for i in range(3): \n",
    "    tmp = df[df.Target == i]\n",
    "    tmp.plot(x=0,y=1, kind='scatter', c=colors[i], ax=axes[0,0])\n",
    "\n",
    "for i in range(3): \n",
    "    tmp = df[df.Target == i]\n",
    "    tmp.plot(x=0,y=2, kind='scatter', c=colors[i], ax=axes[0,1])\n",
    "\n",
    "for i in range(3): \n",
    "    tmp = df[df.Target == i]\n",
    "    tmp.plot(x=0,y=3, kind='scatter', c=colors[i], ax=axes[0,2])\n",
    "    \n",
    "for i in range(3): \n",
    "    tmp = df[df.Target == i]\n",
    "    tmp.plot(x=1,y=2, kind='scatter', c=colors[i], ax=axes[1,0])\n",
    "\n",
    "for i in range(3): \n",
    "    tmp = df[df.Target == i]\n",
    "    tmp.plot(x=1,y=3, kind='scatter', c=colors[i], ax=axes[1,1])\n",
    "\n",
    "for i in range(3): \n",
    "    tmp = df[df.Target == i]\n",
    "    tmp.plot(x=2,y=3, kind='scatter', c=colors[i], ax=axes[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####What do you notice from these plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###KNN with sklearn\n",
    "Now that you should have a decent feel for the Iris data, let's use `sklearn` and **KNN** to try and classify our various flowers.  To do this we'll make use of the `KNeighborsClassifier`, ***and of course we won't forget to cross-validate!***\n",
    "\n",
    "First, let's generate a 70/30 train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets, feature_selection\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "\n",
    "# Create the training (and test) set using scikit-learn's train_test_split function\n",
    "# Retrieve the feature data matrix\n",
    "X = df.iloc[:, 0:3]\n",
    "# Retrieve the target vector\n",
    "y = df['Target']\n",
    "# Perform a 70/30 train/test split for cross-validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "# Note that we also could have just done the following\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=12)\n",
    "\n",
    "# Try this sequence again with the following random seed.\n",
    "# observe how it changes the scores of K quite dramatically\n",
    "# X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try a model, there's one more consideration.  **KNN** takes (at least) 1 parameter, the value of ***k***.  \n",
    "\n",
    "**Q: How do we know what value of *k* to use?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A: **That's easy enough!  We'll just try out a whole range of ***k*** values with **cross-validation** (here a **train/test split**) and choose the model that  yields the best results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a range of possible k values for KNN\n",
    "n_neighbors = range(1, 51, 2)\n",
    "print n_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our k-values to try, let's find a good `KNeighborsClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop through each neighbors value from 1 to 51 and record the model score in scores\n",
    "scores = []\n",
    "for k in n_neighbors:\n",
    "    # Create a KNN Classifier with k=k\n",
    "    clf = neighbors.KNeighborsClassifier(k)\n",
    "    # Fit the KNN Classifier to the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Test the model against the test set and save the score\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the scores that we got against k to see what value of k might be best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot model scores vs. k\n",
    "plt.plot(n_neighbors, scores, linewidth=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####What appears to be the optimal value of k for this dataset?\n",
    "\n",
    "\n",
    "\n",
    "####Why does the classification rate go down with more neighbors?\n",
    "\n",
    "\n",
    "\n",
    "####If we have N points in our dataset, what would happen if we use N neighbors to classify each point?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of k-fold Cross-Validation\n",
    "The work above shows that at 11 neighbors, we can get an ideal result that doesn't overfit the data. To verify this, we'll use automated cross validation in `sklearn`.  Specifically, let's try **5-fold cross-validation** with our **k=11** model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import cross_val\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Create the model with k=11\n",
    "clf = neighbors.KNeighborsClassifier(11, weights='uniform')\n",
    "# Fit the model to the full dataset\n",
    "clf.fit(iris.data, iris.target)\n",
    "# Have sklearn perform 5-fold cross-validation and print out the scores\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "print 'Scores: ' + str(scores)\n",
    "print 'Score Avg: ' + str(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very high scores on this cross-validation show that this is a good model that is likely to generalize well and not overfit.  This is yet another way to do cross-validation with sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualization of the Decision Boundary between Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get a color-coded look at the so-called ***decision boundaries*** for our classifier.  These are the locations (boundaries) in the feature space where the predicted class changes when crossed.  To do this, we'll focus only on the last 2 features of the dataset (if you remember the scatter plots from earlier, these 2 features seemed to do the best job **separating** out the different classes properly).\n",
    "\n",
    "First let's build a **KNN** model with **k=11** using only the last 2 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create our classifier with k=11\n",
    "clf = neighbors.KNeighborsClassifier(11, weights='uniform')\n",
    "# Fit to the data (last 2 features and the target)\n",
    "clf.fit(iris.data[:, 2:4], iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do some manipulations to plot these decision boundaries.  Just take a look at how this works for now, you can read further into the documentation of some of these functions in the links below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = 0.01  # step size in the mesh\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "x_min, y_min = df.min()[['petal length (cm)', 'petal width (cm)']]\n",
    "x_max, y_max = df.max()[['petal length (cm)', 'petal width (cm)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [np.meshgrid](http://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html) (build grid)\n",
    "* [ravel](http://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html) (flatten)\n",
    "* [np.c_](http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.c_.html#numpy.c_)\n",
    "    * `np.c_[np.array([1,2,3]), np.array([4,5,6])]` will get `[[1, 4],[2, 5],[3, 6]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "plt.scatter(df['petal length (cm)'], df['petal width (cm)'], c=iris.target, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = {}, weights = '{}')\".format(clf.n_neighbors, clf.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Oftentimes the features for a given dataset with have different ***scales***.  For varying types of machine learning algorithms, this can cause issues--for instance variables certain variables could tend to dominate leading to potentially unstable models.\n",
    "\n",
    "We can rectify such issues by first scaling our features to all be of comparable scale.  The 2 most common ways to do this are:\n",
    "- **Standardization**: transforming each feature value to the number of standard deviations the value is away from the mean of that feature\n",
    "- **Normalization**: Dividing every feature value by the max (or min) value for that feature (amounts to scaling between 0 and 1)\n",
    "\n",
    "You may see these terms used interchangeable, so all that's important is that you understand what each type is doing and that both are usually acceptable and improve your models.\n",
    "\n",
    "`sklearn` has built-in functions for scaling your variables for you, but you could also perform the same manipulations yourself using `pandas`.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do it with `sklearn`, using the default \"standardization\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm = pd.DataFrame(scale(iris.data), columns=iris.feature_names)\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the results, notice that the standard deviation is now 1 for every field (a consequence of standardization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing with `pandas` manipulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the data fields\n",
    "df_norm = df.iloc[:, 0:4]\n",
    "# Perform the standardization operation\n",
    "df_norm = (df_norm - df_norm.mean())/df_norm.std()\n",
    "# Summarize the results and notice we've done the same thing\n",
    "df_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these exercises we'll try to use **KNN** for a slightly different type of classification task, that of classifying handwritten digits.  KNN is surely not the most adept algorithm to use for this task (neural networks have generally become the standard), but KNN is simple and we can do surprisingly better than random with this simple algorithm.\n",
    "\n",
    "The full dataset that we'll be using is the **MNIST Handwritten Digit** set and can be found [here](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "For these exercises, we'll first manually implement a KNN algorithm so you can better understand KNN.  After that, we'll see how easy it is to do the same work with `sklearn`.\n",
    "\n",
    "###Loading in the Data\n",
    "To this point we've used data where the features were already defined for us, but for the image dataset here we're simply given a bunch of labeled images.  How do we extract numerical features from images?\n",
    "\n",
    "To do this we'll use the pixel values, and thus every image observation will be represented as a vector of pixel values, so there will be `num_pixel` features.\n",
    "\n",
    "The code here handles the task of generating this feature data for you, take a look at it for future reference and understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def download(url):\n",
    "    \"\"\"Download a GZIP archive, return the data as a byte string.\"\"\"\n",
    "    # Do the download by shelling out to curl.\n",
    "    cmd = 'curl \"%s\" | gzip -d' % url\n",
    "    return subprocess.check_output(cmd, shell=True)\n",
    "\n",
    "def get_files():\n",
    "    \"\"\"Download MNIST files from the internet.\"\"\"\n",
    "    url_format = \"http://yann.lecun.com/exdb/mnist/%s-%s-idx%d-ubyte.gz\"\n",
    "    files = [(\"train\", \"images\", 3), (\"train\", \"labels\", 1),\n",
    "             (\"t10k\", \"images\", 3), (\"t10k\", \"labels\", 1)]\n",
    "    \n",
    "    urls = [url_format % values for values in files]\n",
    "    data = [download(url) for url in urls]\n",
    "    return data\n",
    "\n",
    "data = get_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the raw data, the following code will parse it into the data format we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "from numpy import *\n",
    "\n",
    "def parse_labels(data):\n",
    "    \"\"\"Parse labels from the binary file.\"\"\"\n",
    "    \n",
    "    # We're going to use the Python 'struct' package. \n",
    "    # This is an incredibly nice package which allows us to specify the format\n",
    "    # our data is in, and then automatically parses the data from the string.\n",
    "    # Let's start by getting the magic number and the length: the first character\n",
    "    # represents the endianness of the data (in our case, '>' for big endian), while\n",
    "    # the next characters are the format string ('2i' for two integers).\n",
    "    magic, n = struct.unpack_from('>2i', data)\n",
    "    assert magic == 2049, \"Wrong magic number: %d\" % magic\n",
    "    \n",
    "    # Next, let's extract the labels.\n",
    "    labels = struct.unpack_from('>%dB' % n, data, offset=8)\n",
    "    return labels\n",
    "    \n",
    "def parse_images(data):\n",
    "    \"\"\"Parse images from the binary file.\"\"\"\n",
    "    \n",
    "    # Parse metadata.\n",
    "    magic, n, rows, cols = struct.unpack_from('>4i', data)\n",
    "    assert magic == 2051, \"Wrong magic number: %d\" % magic\n",
    "    \n",
    "    # Get all the pixel intensity values.\n",
    "    num_pixels = n * rows * cols\n",
    "    pixels = struct.unpack_from('>%dB' % num_pixels, data, offset=16)\n",
    "    \n",
    "    # Convert this data to a NumPy array for ease of use.\n",
    "    pixels = asarray(pixels, dtype=ubyte)\n",
    "    \n",
    "    # Reshape into actual images instead of a 1-D array of pixels.\n",
    "    images = pixels.reshape((n, cols, rows))\n",
    "    return images\n",
    "\n",
    "train_images = parse_images(data[0])\n",
    "train_labels = parse_labels(data[1])\n",
    "test_images = parse_images(data[2])\n",
    "test_labels = parse_labels(data[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For understanding, let's check out the data we've pulled down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Type?\n",
    "print type(train_images)\n",
    "print type(train_labels)\n",
    "# Dimensions?\n",
    "print train_images.shape\n",
    "print len(train_labels)\n",
    "print test_images.shape\n",
    "print len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "\n",
    "# Get the figure and axes.\n",
    "fig, axes = subplots(5, 5)\n",
    "axes = axes.reshape(25)\n",
    "fig.suptitle(\"Random Sampling of MNIST\")\n",
    "\n",
    "# Plot random images.\n",
    "indices = random.randint(len(train_images), size=25)\n",
    "for axis, index in zip(axes, indices):\n",
    "    image = train_images[index, :, :]\n",
    "    axis.get_xaxis().set_visible(False)\n",
    "    axis.get_yaxis().set_visible(False)\n",
    "    axis.imshow(image, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Building a KNN Classifier\n",
    "We've now stored the image data into `numpy` arrays (which is exactly what we want for our Machine Learning purposes) and we can proceed with building our classifier.\n",
    "\n",
    "####Defining a Distance Function\n",
    "The first requirement for a KNN algorithm is to have a ***distance function/metric***.  This is any function we want to specify that will yield a distance value between any 2 points in our feature space.  \n",
    "\n",
    "For this first implementation, let's use [**Euclidean Distance**](https://en.wikipedia.org/wiki/Euclidean_distance). As an exercise, implement a function that calculates the Euclidean Distance between 2 arrays below: \n",
    "- The function should be called `euclidean_distance`\n",
    "- The parameters should be `img1` and `img2`, for which we'll be expecting `numpy` arrays\n",
    "- The output should return the **squared euclidean distance** (aka don't worry about taking the square root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the euclidean distance function\n",
    "def euclidean_distance(img1, img2):\n",
    "    # Since we're using NumPy arrays, all our operations are automatically vectorized.\n",
    "    # A breakdown of this expression:\n",
    "    #     img1 - img2 is the pixel-wise difference between the images\n",
    "    #     (img1 - img2) ** 2 is the same thing, with each value squared\n",
    "    #     sum((img1 - img2) ** 2) is the sum of the elements in the matrix.\n",
    "    return sum((img1 - img2) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Creating our own NearestNeighborsClassifier class\n",
    "Here we're going to implement a skeleton of the class that will do our classification.\n",
    "- Create a class called `NearestNeighborsClassifier` that inherits from `object`\n",
    "- The class should be initialized by parameters `dataset` and `k`\n",
    "- The class should have a function `predict` that takes a parameter `point` and returns the predicted class value\n",
    "- The class should have a function `distance` that takes 2 parameters `p1` and `p2` and simply calls your `euclidean_distance` function from above on them\n",
    "- The class should have a function `majority_vote` that takes a list of class votes and returns the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class NearestNeighborClassifier(object):\n",
    "    \"\"\"A generic k-nearest neighbor predictor.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, k):\n",
    "        \"\"\"Create a new nearest neighbor classifier.\n",
    "\n",
    "        dataset - a list of data points. Each data point is an (x, y) pair,\n",
    "                  where x is the input and y is the label.\n",
    "        k - the number of neighbors to search for.\"\"\"\n",
    "        # Note how we don't have to do any initialization!\n",
    "        # Once we have a dataset, we can immediately get predictions on new values.\n",
    "        self.dataset = dataset\n",
    "        self.k = k\n",
    "        \n",
    "    def predict(self, point):\n",
    "        # We have to copy the data set list, because once we've located the best\n",
    "        # candidate from it, we don't want to see that candidate again, so we'll delete it.\n",
    "        candidates = self.dataset[:]\n",
    "    \n",
    "        # Get a list of the distances to all other points\n",
    "        distances = [self.distance(x[0], point) for x in candidates]\n",
    "        # Sort the list\n",
    "        distances_sorted = sorted(distances)\n",
    "        # For the lowest k distances, add that candidate to the nearest neighbors\n",
    "        neighbors = []\n",
    "        for i in range(k):\n",
    "            index = distances.index(distances_sorted[i])\n",
    "            neighbors.append(candidates[index])\n",
    "\n",
    "        # Predict by averaging the closets k elements.\n",
    "        prediction = self.majority_vote([value[1] for value in neighbors])\n",
    "        return prediction\n",
    "    \n",
    "    def distance(self, p1, p2):\n",
    "        return euclidean_distance(p1, p2)\n",
    "    \n",
    "    def majority_vote(self, values):\n",
    "        # For convenience, we're going to use a defaultdict.\n",
    "        # This is just a dictionary where values are initialized to zero\n",
    "        # if they don't exist.\n",
    "        counter = defaultdict(int)\n",
    "        for value in values:\n",
    "            # If this weren't a defaultdict, this would error on new vote values.\n",
    "            counter[value] += 1\n",
    "    \n",
    "        # Find out who was the majority.\n",
    "        majority_count = max(counter.values())\n",
    "        for key, value in counter.items():\n",
    "            if value == majority_count:\n",
    "                return key      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our classifier, use the following few cells to:\n",
    "1. Prepare our training and test data in a form our model will like\n",
    "2. Use our model to predict against the test set for varying values of k\n",
    "3. Plot how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert our data set into an easy format to use.\n",
    "# This is a list of (x, y) pairs. x is an image, y is a label.\n",
    "dataset = []\n",
    "for i in xrange(len(train_images)):\n",
    "    dataset.append((train_images[i, :, :], train_labels[i]))\n",
    "    \n",
    "# Create a predictor for various values of k.\n",
    "ks = [1, 2, 3, 4, 5, 6]\n",
    "predictors = [NearestNeighborClassifier(dataset, k) for k in ks] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NOTE: Before Running this cell...\n",
    "Make a duplicate copy of your current notebook via File --> Make a Copy...\n",
    "\n",
    "While the next cell is running, work on the exercises starting with **KNN with sklearn** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_test_set(predictor, test_set):\n",
    "    \"\"\"Compute the prediction for every element of the test set.\"\"\"\n",
    "    predictions = [predictor.predict(test_set[i, :, :]) \n",
    "                   for i in xrange(len(test_set))]\n",
    "    return predictions\n",
    "\n",
    "# Choose a subset of the test set. Otherwise this will never finish.\n",
    "test_set = test_images[0:100, :, :]\n",
    "all_predictions = [predict_test_set(predictor, test_set) for predictor in predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEjCAYAAAA/ugbCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB4GZWcU5CMzMKs5BYGZWcf8PUodtjSoX6usAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x165e43c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_prediction(predictions, answers):\n",
    "    \"\"\"Compute how many were identical in the answers and predictions,\n",
    "    and divide this by the number of predictions to get a percentage.\"\"\"\n",
    "    correct = sum(asarray(predictions) == asarray(answers))\n",
    "    total = float(prod(answers.shape))\n",
    "    return correct / total\n",
    "\n",
    "labels = asarray(test_labels[0:100])\n",
    "accuracies = [evaluate_prediction(pred, labels) for pred in all_predictions]\n",
    "\n",
    "# Draw the figure.\n",
    "fig = figure(1)\n",
    "plt.plot(ks, accuracies, 'ro', figure=fig)\n",
    "\n",
    "fig.suptitle(\"Nearest Neighbor Classifier Accuracies\")\n",
    "fig.axes[0].set_xlabel(\"k (# of neighbors considered)\")\n",
    "fig.axes[0].set_ylabel(\"accuracy (% correct)\");\n",
    "fig.axes[0].axis([0, max(ks) + 1, 0, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###KNN with sklearn\n",
    "While your manually generated KNN process is running, let's see how simple it is to create the exact same classifier with sklearn.\n",
    "\n",
    "####Create a train/test split\n",
    "Use [`sklearn.cross_validation.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) to create a 70/30 train test split of the image data.\n",
    "- Use only the testing data from before (`test_images` and `test_labels`) as your new full dataset\n",
    "- Pass that data to train_test_split\n",
    "- Return the output as `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import sklearn.cross_validation.train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(test_images, test_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the sizes of the resulting sets by calling `shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 28, 28)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have a 3D array, we need to convert everything to a 2D array for sklearn.  We can do this by calling numpy's [`reshape`]() function\n",
    "- Call reshape on `X_train` and `X_test` with parameters `7000` and `28*28` for `X_train` and `3000` and `28*28` for `X_test` to properly format our data arrays\n",
    "- Store these results into `X_train` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 784)\n",
      "(3000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Reshape X_train\n",
    "X_train = X_train.reshape(7000, 28*28)\n",
    "# Reshape X_test\n",
    "X_test = X_test.reshape(3000, 28*28)\n",
    "# Print out the shapes of each now\n",
    "print X_train.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Create and Fit a Model\n",
    "- Import [`sklearn.neighbors.KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "- Create a KNeighborsClassifier object\n",
    "- Use `fit()` to fit the model to the training set\n",
    "- Use `score()` to score the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94399999999999995"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Create Model\n",
    "knn = KNeighborsClassifier()\n",
    "# Fit the model\n",
    "knn.fit(X_train, y_train)\n",
    "# Score the model\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Further Exercises\n",
    "####Changing Distance Metrics\n",
    "1. If you like, try a different distance metric for your manual model by updating the distance function you defined above.  Feel free to google around for possible other distance metrics.\n",
    "2. With sklearn you can set the `metric` parameter in your classifier to any [sklearn.neighbors.DistanceMetric](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html) class.  The default is Euclidean, try experimenting with different DistanceMetrics.\n",
    "\n",
    "####Adding Weights\n",
    "1. In your manual model above, try updating the voting scheme so that votes are only given the weight proportional to the inverse distance.  \n",
    "2. Try doing this same thing in `sklearn` by specifying the `weights` parameter in defining your model.\n",
    "\n",
    "####Varying k\n",
    "Try varying the k-value input into `sklearn` and see if you can find the optimal value.  Optionally, make a plot of the performance vs. different k-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
